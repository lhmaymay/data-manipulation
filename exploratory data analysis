#Spotting mistakes and missing data;
#Mapping out the underlying structure of the data;
#Identifying the most important variables;
#Listing anomalies and outliers;
#Testing a hypotheses / checking assumptions related to a specific model;
#Establishing a parsimonious model (one that can be used to explain the data with minimal predictor variables);
#Estimating parameters and figuring out the associated confidence intervals or margins of error.

#read in data
import pandas as pd
pd.set_option('display.max_column',None)
pd.set_option('display.max_rows',None)
pd.set_option('display.max_seq_items',None)
pd.set_option('display.max_colwidth', 1000)
pd.set_option('expand_frame_repr', True)
df=pd.read_csv('valid_features.csv.gz',compression='gzip',index_col=0)
df.to_csv('valid_features.csv.gz',compression='gzip',index=False)

# read in data from S3 bucket
import boto3
from sagemaker import get_execution_role
import numpy as np
import glob
import pandas as pd
import numpy as np
import csv
import glob
import h5py
import dask.dataframe as dd
from common.constant import CONTROL_A_DELIM
from common.constant import FRAUD_FLAG
from common.constant import NA_DELIM
role = get_execution_role()
bucket='ds/data/transformed/training'
data_key = 'transformed.training.*.gz'
data_files_location = 's3://{}/{}'.format(bucket, data_key)
x_data = dd.read_csv(data_files_location, delimiter=CONTROL_A_DELIM, compression='gzip',quoting=csv.QUOTE_NONE, header=0,
                         na_values=NA_DELIM, keep_default_na=False, dtype=str, blocksize=None)
x_data = x_data.compute()
train=pd.merge(x_train,x_data,on='ind',how='inner')


data_key = 'valid_extra.csv.gz'
data_files_location = 's3://{}/{}'.format(bucket, data_key)
df_valid= pd.read_csv(data_files_location, compression='gzip')
df_valid.head()

# read in snappy files
import snappy
with open (snappy_file, "r") as input_file:
  data = input_file.read()
  decompressor = snappy.hadoop_snappy.StreamDecompressor()
  uncompressed = decompressor.decompress(data)
 

# 0: data quality, sanity check
df.isnull().values.any()
df.isnull().sum().max()

# check missing value % missing, data completeness
percent_missing =x_train.isnull().sum() * 100 / len(x_train)
missing_value_df = pd.DataFrame({'column_name': x_train.columns,
                                 'percent_missing': percent_missing})
missing_value_df.sort_values('percent_missing', inplace=True,ascending=False)
missing_value_df

x_train=x_train.drop(droplist_missing,axis=1)

# transform string to numeric
for col in x_train.columns:
    x_train[col]=pd.to_numeric(x_train[col])
x_train.dtypes

# transform numeric to intervals
def ff(row):
    if row['risk_score']>=0 and row['risk_score']<=100:
        val = '0-100'
    elif row['risk_score']>100 and row['risk_score']<=200:
        val = '101-200'
    elif row['risk_score']>200 and row['risk_score']<=300:
        val = '201-300'
    elif row['risk_score']>300 and row['risk_score']<=400:
        val = '301-400'
    elif row['risk_score']>400:
        val = '401-500'
    else:
        val='Missing'
    return val 
df['risk_score_band'] = df.apply(ff, axis=1)

# 1: Univariate visualization — provides summary statistics for each field in the raw data set

count_classes = df['Class'].value_counts(sort = True, normalize=True)
count_classes.plot(kind = 'bar', rot=0)
plt.title("Transaction class distribution")
plt.xticks(range(2), LABELS)
plt.xlabel("Class")
plt.ylabel("Frequency");


# 2: Bivariate visualization — is performed to find the relationship between each variable in the dataset and the target variable of interest
# check correlations between features
# check correlations with target variable
grouped = x_valid[['risk_score', 'fraudflag']].groupby('risk_score').mean().reset_index()
hist_kws={'histtype': 'bar', 'edgecolor':'black', 'alpha': 0.2}

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))
sns.distplot(x_valid[x_valid['fraudflag'] == 0]['risk_score'], label='Non-Fraud', 
             ax=ax[0], hist_kws=hist_kws)
sns.distplot(x_valid[x_valid['fraudflag'] == 1]['risk_score'], label='Fraud', 
             ax=ax[0], hist_kws=hist_kws)
ax[0].set_title('Count Plot of Risk Score', fontsize=16)
ax[0].legend()
ax[1].plot(grouped['risk_score'], grouped['fraudflag'], '.-')
ax[1].set_title('Mean Fraud Rate vs. Risk Score', fontsize=16)
ax[1].set_xlabel(' Risk Score')
ax[1].set_ylabel('Mean fraud rate')
ax[1].grid(True)
plt.show()


# pivot table
pd.pivot_table(df,index=['risk_score_band'],columns=['product_type'],values=['fraudflag'],aggfunc={np.mean,len})

# heat map triangle shape of correlation matrix
import seaborn as sns
sns.set(style="white")
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True
# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))
# Generate a custom diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)
# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})
plt.show()
     
     

# 3: Multivariate visualization — is performed to understand interactions between different fields in the dataset
# 4: Dimensionality reduction — helps to understand the fields in the data that account for the most variance between observations and allow for the processing of a reduced volume of data.
# help you to create graphical displays of high-dimensional data containing many variables;


#5: Multivariate visualizations, for mapping and understanding interactions between different fields in the data;
#6: K-Means Clustering (creating “centres” for each cluster, based on the nearest mean);
#7: Predictive models, e.g. linear regression.


Typical graphical techniques used in EDA are:

# (1) Box plot:
import seaborn as sns
sns.boxplot(x='product_type',y='tte',hue='fraudflag',data=df)


# Python Pandas add column for row-wise max value of selected columns
frame['HighScore'] = frame[['test1','test2','test3']].max(axis=1)

(2) Histogram
# Plotting a Histogram
df.Make.value_counts().nlargest(40).plot(kind=’bar’, figsize=(10,5))
plt.title(“Number of cars by make”)
plt.ylabel(‘Number of cars’)
plt.xlabel(‘Make’);

(3) Heat map
# Finding the relations between the variables.
plt.figure(figsize=(20,10))
c= df.corr()
sns.heatmap(c,cmap=”BrBG”,annot=True)
c

(4) Scatter plot
# Plotting a scatter plot
fig, ax = plt.subplots(figsize=(10,6))
ax.scatter(df[‘HP’], df[‘Price’])
ax.set_xlabel(‘HP’)
ax.set_ylabel(‘Price’)
plt.show()






Multi-vari chart
Run chart
Pareto chart
Scatter plot
Stem-and-leaf plot
Parallel coordinates
Odds ratio
Targeted projection pursuit
Glyph-based visualization methods such as PhenoPlot[8] and Chernoff faces
Projection methods such as grand tour, guided tour and manual tour
Interactive versions of these plots
Dimensionality reduction:

Multidimensional scaling
Principal component analysis (PCA)
Multilinear PCA
Nonlinear dimensionality reduction (NLDR)

Typical quantitative techniques are:

Median polish
Trimean
Ordination
